{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WARNING\n",
    "# DO NOT RUN THIS NOTEBOOK UNTIL YOU ARE DONE WITH THE REST OF THE WORKSHOP\n",
    "THIS WILL INSTALL TENSORFLOW 2.0 WHICH WILL MESS UP THE REST OF THE ENVIRONMENT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf keras-tuner\n",
    "git clone https://github.com/keras-team/keras-tuner.git\n",
    "cd keras-tuner\n",
    "git reset --hard fa47f90729237d255f41a654bb174822b43a391b\n",
    "pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y tensorflow==1.13.1\n",
    "!pip install tensorflow==2.0.0-beta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner.engine.hypermodel import HyperModel\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "\n",
    "\n",
    "(x, y), (val_x, val_y) = keras.datasets.mnist.load_data()\n",
    "x = x.astype('float32') / 255.\n",
    "val_x = val_x.astype('float32') / 255.\n",
    "\n",
    "x = x[:10000]\n",
    "y = y[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to perform hyperparameter tuning for a single-layer dense neural network using random search.\n",
    "\n",
    "First, we define a model-building function. It takes an argument hp from which you can sample hyperparameters, such as hp.Range('units', min_value=32, max_value=512, step=32) (an integer from a certain range)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case #1:  Basic\n",
    "- Define a `build_model` function\n",
    "- Returns a compiled model\n",
    "- Use hyperparameters defined on the fly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The search space may contain conditional hyperparameters.\n",
    "\n",
    "Below, we have a for loop creating a tunable number of layers, which themselves involve a tunable units parameter.\n",
    "\n",
    "This can be pushed to any level of parameter interdependency, including recursion.\n",
    "\n",
    "Note that all parameter names should be unique (here, in the loop over i, we name the inner parameters 'units_' + str(i))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Flatten(input_shape=(28, 28)))\n",
    "    for i in range(hp.Range('num_layers', 2, 20)):\n",
    "        model.add(layers.Dense(units=hp.Range('units_' + str(i), 32, 512, 32),\n",
    "                               activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, instantiate a tuner. You should specify the model-building function, the name of the objective to optimize (whether to minimize or maximize is automatically inferred for built-in metrics), the total number of trials (max_trials) to test, and the number of models that should be built and fit for each trial (executions_per_trial).\n",
    "\n",
    "Available tuners are RandomSearch and Hyperband.\n",
    "\n",
    "Note: the purpose of having multiple executions per trial is to reduce results variance and therefore be able to more accurately assess the performance of a model. If you want to get results faster, you could set executions_per_trial=1 (single round of training for each model configuration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=3,\n",
    "    directory='tuner-results',\n",
    "    project_name='helloworld_case_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can print a summary of the search space:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, start the search for the best hyperparameter configuration. The call to search has the same signature as model.fit().\n",
    "\n",
    "Here's what happens in search: models are built iteratively by calling the model-building function, which populates the hyperparameter space (search space) tracked by the hp object. The tuner progressively explores the space, recording metrics for each configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(x=x,\n",
    "             y=y,\n",
    "             epochs=1,\n",
    "             validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When search is over, you can retrieve the best model(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = tuner.get_best_models(num_models=2)\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or print a summary of the results:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also find detailed logs and checkpoints in the folder `tuner-results`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -al tuner-results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case #2:\n",
    "- Override the loss and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(name='my_loss'),\n",
    "    metrics=['accuracy', 'mse'],\n",
    "    max_trials=5,\n",
    "    directory='tuner-results',\n",
    "    project_name='helloworld_case_2')\n",
    "\n",
    "tuner.search(x, y,\n",
    "             epochs=1,\n",
    "             validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case #3:\n",
    "- We define a custom HyperModel subclass instead of model-building function\n",
    "- This makes it easy to share and reuse hypermodels.\n",
    "- A HyperModel subclass only needs to implement a build(self, hp) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHyperModel(HyperModel):\n",
    "\n",
    "    def __init__(self, img_size, num_classes):\n",
    "        self.img_size = img_size\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.Flatten(input_shape=self.img_size))\n",
    "        for i in range(hp.Range('num_layers', 2, 20)):\n",
    "            model.add(layers.Dense(units=hp.Range('units_' + str(i), 32, 512, 32),\n",
    "                                   activation='relu'))\n",
    "        model.add(layers.Dense(self.num_classes, activation='softmax'))\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(\n",
    "                hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = RandomSearch(\n",
    "    MyHyperModel(img_size=(28, 28), num_classes=10),\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    directory='tuner-results',\n",
    "    project_name='helloworld_case_3')\n",
    "\n",
    "tuner.search(x,\n",
    "             y=y,\n",
    "             epochs=1,\n",
    "             validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case #4:\n",
    "- Restrict the search space\n",
    "- Use default values for params that are left out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = HyperParameters()\n",
    "hp.Choice('learning_rate', [1e-1, 1e-3])\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    max_trials=5,\n",
    "    hyperparameters=hp,\n",
    "    tune_new_entries=False,\n",
    "    objective='val_accuracy',\n",
    "    directory='tuner-results',\n",
    "    project_name='helloworld_case_4')\n",
    "\n",
    "tuner.search(x=x,\n",
    "             y=y,\n",
    "             epochs=1,\n",
    "             validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case #5:\n",
    "- We override specific parameters with fixed values that aren't the default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = HyperParameters()\n",
    "hp.Fixed('learning_rate', 0.1)\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    max_trials=5,\n",
    "    hyperparameters=hp,\n",
    "    tune_new_entries=True,\n",
    "    objective='val_accuracy',\n",
    "    directory='tuner-results',\n",
    "    project_name='helloworld_case_5')\n",
    "\n",
    "tuner.search(x=x,\n",
    "             y=y,\n",
    "             epochs=1,\n",
    "             validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case #6:\n",
    "- We reparameterize the search space\n",
    "- This means that we override the distribution of specific hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = HyperParameters()\n",
    "hp.Choice('learning_rate', [1e-1, 1e-3])\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    max_trials=5,\n",
    "    hyperparameters=hp,\n",
    "    tune_new_entries=True,\n",
    "    objective='val_accuracy',\n",
    "    directory='tuner-results',\n",
    "    project_name='helloworld_case_6')\n",
    "\n",
    "tuner.search(x=x,\n",
    "             y=y,\n",
    "             epochs=1,\n",
    "             validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case #7:\n",
    "- We predefine the search space\n",
    "- No unregistered parameters are allowed in `build`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = HyperParameters()\n",
    "hp.Choice('learning_rate', [1e-1, 1e-3])\n",
    "hp.Range('num_layers', 2, 20)\n",
    "\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Flatten(input_shape=(28, 28)))\n",
    "    for i in range(hp.get('num_layers')):\n",
    "        model.add(layers.Dense(32,\n",
    "                               activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(hp.get('learning_rate')),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    max_trials=5,\n",
    "    hyperparameters=hp,\n",
    "    allow_new_entries=False,\n",
    "    objective='val_accuracy',\n",
    "    directory='tuner-results',\n",
    "    project_name='helloworld_case_7')\n",
    "\n",
    "tuner.search(x=x,\n",
    "             y=y,\n",
    "             epochs=1,\n",
    "             validation_data=(val_x, val_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
